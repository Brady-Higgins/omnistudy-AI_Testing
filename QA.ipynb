{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf21372e-5ae9-4147-9e4c-0d63cbb53dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brady\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "#overide = true just forces a reload on the .env file in case api key changes\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Access the API key\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "huggingface_api_token = os.getenv(\"HUGGING_FACE_API_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5083a36-6d22-4172-9e73-3aa00daaeddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Brady\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize the pinecone index\n",
    "import pinecone      \n",
    "\n",
    "pinecone.init(      \n",
    "\tapi_key=pinecone_api_key,      \n",
    "\tenvironment='gcp-starter'      \n",
    ")      \n",
    "index = pinecone.Index('haystack')\n",
    "\n",
    "#load a pinecone document store object with the index defined previously\n",
    "from haystack.document_stores import PineconeDocumentStore\n",
    "\n",
    "document_store = PineconeDocumentStore(\n",
    "    api_key=pinecone_api_key,\n",
    "    pinecone_index=index,\n",
    "    similarity=\"cosine\",\n",
    "    embedding_dim=768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348ebde1-21da-479b-8dd9-82924eb5db00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brady\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "#Initialize retriever model\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\",\n",
    "    model_format=\"sentence_transformers\"\n",
    ")\n",
    "from haystack.pipelines import DocumentSearchPipeline\n",
    "\n",
    "search_pipe = DocumentSearchPipeline(retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25476a91-4f84-4576-b26b-4543d201d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is big O notation?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05f9a604-9d3d-4438-94ed-a4d22120aeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88421a6182d042239a64cf86b4fff8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = search_pipe.run(query=query, params={\"Retriever\": {\"top_k\": 2}})['documents']\n",
    "documents = []\n",
    "for doc in docs:\n",
    "    documents.append(doc.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "120353d4-b737-43d8-b10e-4ffcfc541795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supporting documents provided\n",
      "For example, the time to paint a fence that's w meters\n",
      "wide and h meters high could be described as O ( wh). If you needed p layers of paint, then you could say\n",
      "that the time is O ( whp).\n",
      "Big 0, Big Theta, and Big Omega\n",
      "If you've never covered big O in an academic setting, you can probably skip this subsection. It might\n",
      "confuse you more than it helps. This \"FYI\" is mostly here to clear up ambiguity in wording for people who\n",
      "have learned big O before, so that they don't say, \"But I thought big O meant..:'\n",
      "Academics use big 0, big 0 (theta), and big O (omega) to describe runtimes.\n",
      "O (big 0): In academia, big O describes an upper bound on the time. An algorithm that prints all the\n",
      "values in an array could be described as O(N), but it could also be described as O(N2), O(N3), or 0( 2N)\n",
      "(or many other big O times). The algorithm is at least as fast as each of these; therefore they are upper\n",
      "bounds on the runtime. This is similar to a less-than-or-equal-to relationship. If Bob is X years old (I'll\n",
      "assume no one lives past age 130), then you could say X .$. 130. It would also be correct to say that\n",
      "X .$. 1, 000 or X .$. 1,000,000. It's technically true (although not terribly useful). \n",
      "CrackingTheCodinglnterview.com \\ 6th Edition\f",
      "VI\n",
      "BigO\n",
      "This is such an important concept that we are dedicating an entire (long!) chapter to it.\n",
      "Big O time is the language and metric we use to describe the efficiency of algorithms. Not understanding\n",
      "it thoroughly can really hurt you in developing an algorithm. Not only might you be judged harshly for\n",
      "not really understanding big 0, but you will also struggle to judge when your algorithm is getting faster or\n",
      "slower.\n",
      "Master this concept.\n",
      "\u0011 An Analogy\n",
      "Imagine the following scenario: You've got a file on a hard drive and you need to send it to your friend who\n",
      "lives across the country. You need to get the file to your friend as fast as possible. How should you send it?\n",
      "Most people's first thought would be email, FTP, or some other means of electronic transfer. That thought is\n",
      "reasonable, but only half correct.\n",
      "If it's a small file, you're certainly right. It would take 5 - 10 hours to get to an airport, hop on a flight, and\n",
      "then deliver it to your friend.\n",
      "But what if the file were really, really large? Is it possible that it's faster to physically deliver it via plane?\n",
      "Yes, actually it is. A one-terabyte (1 TB) file could take more than a day to transfer electronically. It would be\n",
      "much faster to just fly it across the country. \n"
     ]
    }
   ],
   "source": [
    "print(\"supporting documents provided\")\n",
    "for text in documents:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8197d219-0f8b-4726-84ad-b34e5fe55028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Big O is an upper bound on the time it takes to do something. For example, the time to paint a fence that's w meters wide and h meters high could be described as O (wh). If you needed p layers of paint, then you could say that the time is O ( whp). Theta and Omega are upper bounds on the runtimes of algorithms.\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"vblagoje/bart_lfqa\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "conditioned_doc = \"<P> \" + \" <P> \".join([d for d in documents])\n",
    "query_and_docs = \"question: {} context: {}\".format(query, conditioned_doc)\n",
    "\n",
    "model_input = tokenizer(query_and_docs, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "generated_answers_encoded = model.generate(input_ids=model_input[\"input_ids\"].to(device),\n",
    "                                           attention_mask=model_input[\"attention_mask\"].to(device),\n",
    "                                           min_length=64,\n",
    "                                           max_length=256,\n",
    "                                           do_sample=False, \n",
    "                                           early_stopping=True,\n",
    "                                           num_beams=8,\n",
    "                                           temperature=1.0,\n",
    "                                           top_k=None,\n",
    "                                           top_p=None,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           no_repeat_ngram_size=3,\n",
    "                                           num_return_sequences=1)\n",
    "tokenizer.batch_decode(generated_answers_encoded, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dca0850-99d7-45b2-b21b-64fe8c25155c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c6588804de4d71becc7da321b3949e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test your code-on paper']\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser\n",
    "\n",
    "query = \"How to prepare for coding interviews?\"\n",
    "\n",
    "prompt_node = PromptNode()\n",
    "output = prompt_node.prompt(prompt_template=\"deepset/question-answering\",\n",
    "                   query=query,\n",
    "                   documents=search_pipe.run(query=query,params={\"Retriever\": {\"top_k\": 1}})['documents'])\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36384513-3f55-4846-89ae-fada2929b887",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4fd913e-1e28-4faf-be44-fc6029df95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser\n",
    "from haystack import Document\n",
    "\n",
    "pipe.add_node(component=node,name=\"prompt_node\",inputs=[\"Query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a3da067-358f-4996-8e88-689bee292549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca52f2b481640dfa891a061b8a5c9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Exception",
     "evalue": "Exception while running node 'prompt_node': HuggingFace Inference returned an error.\nStatus code: 403\nResponse body: {\"error\":\"The model google/flan-t5-xl is too large to be loaded automatically (11GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\"}\nEnable debug logging to see the data that was passed when the pipeline failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\haystack\\nodes\\prompt\\invocation_layer\\hugging_face_inference.py:237\u001b[0m, in \u001b[0;36mHFInferenceEndpointInvocationLayer._post\u001b[1;34m(self, data, stream, attempts, status_codes_to_retry, timeout)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequest_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatus_codes_to_retry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatus_codes_to_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattempts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattempts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\haystack\\utils\\requests_utils.py:93\u001b[0m, in \u001b[0;36mrequest_with_retry\u001b[1;34m(attempts, status_codes_to_retry, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# We raise here too in case the request failed with a status code that\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# won't trigger a retry, this way the call will still cause an explicit exception\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/google/flan-t5-xl",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mHuggingFaceInferenceError\u001b[0m                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\haystack\\pipelines\\base.py:567\u001b[0m, in \u001b[0;36mPipeline.run\u001b[1;34m(self, query, file_paths, labels, documents, meta, params, debug)\u001b[0m\n\u001b[0;32m    566\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 567\u001b[0m node_output, stream_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_debug\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m node_output \u001b[38;5;129;01mand\u001b[39;00m node_id \u001b[38;5;129;01min\u001b[39;00m node_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_debug\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\haystack\\pipelines\\base.py:469\u001b[0m, in \u001b[0;36mPipeline._run_node\u001b[1;34m(self, node_id, node_input)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_id: \u001b[38;5;28mstr\u001b[39m, node_input: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mnodes[node_id][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39m_dispatch_run(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnode_input)\n",
      "File \u001b[1;32m~\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\haystack\\nodes\\base.py:201\u001b[0m, in \u001b[0;36mBaseComponent._dispatch_run\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03mThe Pipelines call this method when run() is executed. This method in turn executes the _dispatch_run_general()\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03mmethod with the correct run method.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_run_general(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\haystack\\nodes\\base.py:245\u001b[0m, in \u001b[0;36mBaseComponent._dispatch_run_general\u001b[1;34m(self, run_method, **kwargs)\u001b[0m\n\u001b[0;32m    243\u001b[0m         run_inputs[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m--> 245\u001b[0m output, stream \u001b[38;5;241m=\u001b[39m run_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrun_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrun_params)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# Collect debug information\u001b[39;00m\n",
      "File \u001b[1;32m~\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\haystack\\nodes\\prompt\\prompt_node.py:312\u001b[0m, in \u001b[0;36mPromptNode.run\u001b[1;34m(self, query, file_paths, labels, documents, meta, invocation_context, prompt_template, generation_kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m invocation_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare(\n\u001b[0;32m    309\u001b[0m     query, file_paths, labels, documents, meta, invocation_context, prompt_template, generation_kwargs\n\u001b[0;32m    310\u001b[0m )\n\u001b[1;32m--> 312\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minvocation_context, prompt_collector\u001b[38;5;241m=\u001b[39mprompt_collector)\n\u001b[0;32m    314\u001b[0m prompt_template_resolved: PromptTemplate \u001b[38;5;241m=\u001b[39m invocation_context\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_template\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\haystack\\nodes\\prompt\\prompt_node.py:140\u001b[0m, in \u001b[0;36mPromptNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_template\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt(prompt_template, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\haystack\\nodes\\prompt\\prompt_node.py:169\u001b[0m, in \u001b[0;36mPromptNode.prompt\u001b[1;34m(self, prompt_template, *args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt being sent to LLM with prompt \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and kwargs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, prompt, kwargs_copy)\n\u001b[1;32m--> 169\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_model\u001b[38;5;241m.\u001b[39minvoke(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_copy)\n\u001b[0;32m    170\u001b[0m results\u001b[38;5;241m.\u001b[39mextend(output)\n",
      "File \u001b[1;32m~\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\haystack\\nodes\\prompt\\prompt_model.py:129\u001b[0m, in \u001b[0;36mPromptModel.invoke\u001b[1;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03mTakes in a prompt and returns a list of responses using the underlying invocation layer.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m:return: A list of model-generated responses for the prompt or prompts.\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_invocation_layer\u001b[38;5;241m.\u001b[39minvoke(prompt\u001b[38;5;241m=\u001b[39mprompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\haystack\\nodes\\prompt\\invocation_layer\\hugging_face_inference.py:173\u001b[0m, in \u001b[0;36mHFInferenceEndpointInvocationLayer.invoke\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs_with_defaults\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs_with_defaults\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwatermark\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs_with_defaults\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwatermark\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    172\u001b[0m }\n\u001b[1;32m--> 173\u001b[0m response: requests\u001b[38;5;241m.\u001b[39mResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[1;32m~\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\haystack\\nodes\\prompt\\invocation_layer\\hugging_face_inference.py:254\u001b[0m, in \u001b[0;36mHFInferenceEndpointInvocationLayer._post\u001b[1;34m(self, data, stream, attempts, status_codes_to_retry, timeout)\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m HuggingFaceInferenceUnauthorizedError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI key is invalid: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HuggingFaceInferenceError(\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFace Inference returned an error.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStatus code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResponse body: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         status_code\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mstatus_code,  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mHuggingFaceInferenceError\u001b[0m: HuggingFace Inference returned an error.\nStatus code: 403\nResponse body: {\"error\":\"The model google/flan-t5-xl is too large to be loaded automatically (11GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\"}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat kind of Big O notation is used during a coding interview?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m res\u001b[38;5;241m=\u001b[39m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRetriever\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\haystack\\pipelines\\base.py:574\u001b[0m, in \u001b[0;36mPipeline.run\u001b[1;34m(self, query, file_paths, labels, documents, meta, params, debug)\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# The input might be a really large object with thousands of embeddings.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;66;03m# If you really want to see it, raise the log level.\u001b[39;00m\n\u001b[0;32m    573\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while running node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with input \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, node_id, node_input)\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while running node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEnable debug logging to see the data that was passed when the pipeline failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    577\u001b[0m queue\u001b[38;5;241m.\u001b[39mpop(node_id)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: Exception while running node 'prompt_node': HuggingFace Inference returned an error.\nStatus code: 403\nResponse body: {\"error\":\"The model google/flan-t5-xl is too large to be loaded automatically (11GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\"}\nEnable debug logging to see the data that was passed when the pipeline failed."
     ]
    }
   ],
   "source": [
    "query = \"What kind of Big O notation is used during a coding interview?\"\n",
    "res=pipe.run(query=query,documents=search_pipe.run(query=query,params={\"Retriever\": {\"top_k\": 2}})['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faef3e6-00dd-4eac-ba2a-d1027a928570",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res['answers'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
