Pinecone Document Store
max size of a vector = 20MB
Vector: a text chunk of a document w/ it's value being determined by word and sentence similiarity calculations
FireBase: hosting ML models and storing documents
1GB stored
Set a cap on user uploads to 10MB with limited feature access

Namespaces:
A unique namespace will be used per each user:
Pros: the nlp will only pull info from the users documents they've uploaded (increasing speed and removing the possibility of data corruption with conflicting texts)
cons: multiple of the same textbooks potentially in the database
no maximum unless running the gcp-starter (free version)  //this would imply a maximum of 100 users
alt: Each book could occupy a namespace and users could then prompt a specific book they're asking questions from (implies: 100 books in system max if we're free version)
paid version is $850 per 365 days (if running 24/7)

Haystack:
we're using this for the embeddings, retriever, and reader model
Retriever:
Embedding retriever: generates documents based on sentence similiarity
Dense retriever: not using, but does dense calculations to determine similiarity (max length of 100 tokens)


Potential Issues: 
LLM model needs to be altered
model needs: high token limit (sometimes changable within) and high scores
potential switch to RAG


Integration Into Omnistudy Project:
Create an index
distribute a .env w/ api_key
add py files

Struc:

haystack_embedder
input: location of file
output: nothing
effect: will upload textbook to pinecone index

haystack_DocStore
input:none
output: document store object
effect: document store object for manipulation

QA_pipeline
input: Book name
output: QA pipeline object 

Question_Gen_Pipeline
input: Book name
output: Question_Gen pipeline object