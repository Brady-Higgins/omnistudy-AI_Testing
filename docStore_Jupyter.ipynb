{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e26760-9d58-4141-a848-a47f8fe24f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Issues to address\n",
    "#figure out optimal stride and text_length\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3312e772-7a60-4e07-b20d-0c67c79a44c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brady\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "#overide = true just forces a reload on the .env file in case api key changes\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Access the API key\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1266faff-c62b-4819-81ec-789946e899fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the pinecone index\n",
    "import pinecone      \n",
    "\n",
    "pinecone.init(      \n",
    "\tapi_key=api_key,      \n",
    "\tenvironment='gcp-starter'      \n",
    ")      \n",
    "index = pinecone.Index('haystack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf61bdd-f995-4044-a3a3-57988d90b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a pinecone document store object with the index defined previously\n",
    "from haystack.document_stores import PineconeDocumentStore\n",
    "\n",
    "document_store = PineconeDocumentStore(\n",
    "    api_key=api_key,\n",
    "    pinecone_index=index,\n",
    "    similarity=\"cosine\",\n",
    "    embedding_dim=768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ae846-f33a-41c2-a25d-df59c56ce964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Textbook Extraction to make digestable for docu store\n",
    "from TextBookExctraction import Process_PDF\n",
    "#max_chunk_length is the max token length of each vector within the database\n",
    "#stride refers to the step taken to find the middle of each vector. \n",
    "#If stride is 2 and if max_length is 3, we move 2 steps forwards and each vector will contain 3 tokens with an overlap of 1\n",
    "# [1,2,3] , [3,4,5], [5,6,7], ... , [n-1,n,n+1]            with each array referring to a chunk/vector\n",
    "pdf_processor = Process_PDF(pdf_path=\"./Textbooks/CrackingTheCodingInterview.pdf\")\n",
    "text = pdf_processor.extract_text_from_pdf()\n",
    "cleaned_text = pdf_processor.preprocess_text(text)\n",
    "text_chunks = pdf_processor.segment_text(cleaned_text, max_chunk_length=500, stride=400)\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b401fa-dcce-464a-b205-63ce8d20ad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brady\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#Initialize retriever model\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\",\n",
    "    model_format=\"sentence_transformers\",\n",
    ")\n",
    "\n",
    "# import torch\n",
    "# #Initialize retriever model\n",
    "# from haystack.nodes import EmbeddingRetriever\n",
    "# retriever = EmbeddingRetriever(\n",
    "#     document_store=document_store,\n",
    "#     embedding_model=\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\",\n",
    "#     model_format=\"sentence_transformers\",\n",
    "#     top_k=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7d7b2-f1f4-4dcf-b076-935b913315bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document\n",
    "\n",
    "batch_size = 256\n",
    "total_doc_count = len(text_chunks)\n",
    "\n",
    "counter = 0\n",
    "docs = []\n",
    "for d in text_chunks:\n",
    "    doc = Document(\n",
    "        content = d\n",
    "    )\n",
    "    docs.append(doc)\n",
    "    counter += 1\n",
    "    if counter % batch_size == 0 or counter == total_doc_count:\n",
    "        embeds = retriever.embed_documents(docs)\n",
    "        for i, doc in enumerate(docs):\n",
    "            doc.embedding = embeds[i]\n",
    "        document_store.write_documents(docs)\n",
    "        docs.clear()\n",
    "    if counter == total_doc_count:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d9bdc8-c6e1-4b0e-8391-7c04b7249b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80f3047b48d4c0c911a981772797fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "s to explore what areas of technology you're familiar with. \n",
      "Next, you fly to Seattle (or whichever office you're interviewing for) for four or five interviews with one or \n",
      "two teams that have selected you based on your resume and phone interviews. You will have to code on a \n",
      "whiteboard, and some interviewers will stress other skills. Interviewers are each assigned a specific area to \n",
      "probe and may seem very different from each other. They cannot see the other feedback until they have \n",
      "submitted\n"
     ]
    }
   ],
   "source": [
    "#example usage of retriever\n",
    "from haystack.pipelines import DocumentSearchPipeline\n",
    "from haystack.utils import print_documents\n",
    "\n",
    "search_pipe = DocumentSearchPipeline(retriever)\n",
    "result = search_pipe.run(\n",
    "    query=\"what happens during a coding interview?\",\n",
    "    params={\"Retriever\": {\"top_k\": 2}}\n",
    ")\n",
    "print(type(result))\n",
    "\n",
    "content_pieces = [document.content for document in result['documents']]\n",
    "print(content_pieces[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38dda222-c877-43f7-af3b-d73debe79aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a710f0d3a44d4e57bd3a9a6f62bcb431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brady\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Brady\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e3986c17cf4705b0736d38873fcb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbc074c84934288b349cc809665d85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9db01aff7540b782d1481a50a19768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b4b288ec974d108e8703417ab330f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14bbdf2dafc54962afa0ad97316c485a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621fbac38561416982cc9d81092e5133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser\n",
    "\n",
    "lfqa_prompt = PromptTemplate(\n",
    "    prompt=\"\"\"Synthesize a comprehensive answer from the following text for the given question.\n",
    "                             Provide a clear and concise response that summarizes the key points and information presented in the text.\n",
    "                             Your answer should be in your own words and be no longer than 50 words.\n",
    "                             \\n\\n Related text: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\"\"\",\n",
    "                             output_parser=AnswerParser(),\n",
    ")\n",
    "\n",
    "prompt_node = PromptNode( default_prompt_template=lfqa_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68452bd0-feab-46e7-84be-4635e4823f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Document: id=3a4a4200b73a0aab07f1c7cbd9caa735, content='(and many other companies). algorithm and coding problems form the \n",
      "largest component of the intervi...'>\n"
     ]
    }
   ],
   "source": [
    "print(Document(content_pieces[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b2fb9f-22b1-4d97-bc90-a87f2aaba1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import Pipeline\n",
    "from haystack.schema import Document\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# pipeline.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "pipeline.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"Query\"])\n",
    "output = pipeline.run(query=\"what happens during a coding interview?\", documents=[Document(content_pieces[0]),Document(content_pieces[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f2fddc7-91bf-47fb-bc08-68f881c1a840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You will have to code on a whiteboard, and some interviewers will stress other skills.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.answer for a in output[\"answers\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
