{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e26760-9d58-4141-a848-a47f8fe24f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Issues to address\n",
    "#figure out optimal stride and text_length\n",
    "#apply namespaces\n",
    "#manage which textbooks are known\n",
    "#build actual pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3312e772-7a60-4e07-b20d-0c67c79a44c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brady\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "#overide = true just forces a reload on the .env file in case api key changes\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Access the API key\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "huggingface_api_token = os.getenv(\"HUGGING_FACE_API_TOKEN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1266faff-c62b-4819-81ec-789946e899fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the pinecone index\n",
    "import pinecone      \n",
    "\n",
    "pinecone.init(      \n",
    "\tapi_key=pinecone_api_key,      \n",
    "\tenvironment='gcp-starter'      \n",
    ")      \n",
    "index = pinecone.Index('haystack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf61bdd-f995-4044-a3a3-57988d90b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a pinecone document store object with the index defined previously\n",
    "from haystack.document_stores import PineconeDocumentStore\n",
    "\n",
    "document_store = PineconeDocumentStore(\n",
    "    api_key=pinecone_api_key,\n",
    "    pinecone_index=index,\n",
    "    similarity=\"cosine\",\n",
    "    embedding_dim=768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ae846-f33a-41c2-a25d-df59c56ce964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Textbook Extraction to make digestable for docu store\n",
    "from TextBookExctraction import Process_PDF\n",
    "#max_chunk_length is the max token length of each vector within the database\n",
    "#stride refers to the step taken to find the middle of each vector. \n",
    "#If stride is 2 and if max_length is 3, we move 2 steps forwards and each vector will contain 3 tokens with an overlap of 1\n",
    "# [1,2,3] , [3,4,5], [5,6,7], ... , [n-1,n,n+1]            with each array referring to a chunk/vector\n",
    "pdf_processor = Process_PDF(pdf_path=\"./Textbooks/CrackingTheCodingInterview.pdf\")\n",
    "text = pdf_processor.extract_text_from_pdf()\n",
    "cleaned_text = pdf_processor.preprocess_text(text)\n",
    "text_chunks = pdf_processor.segment_text(cleaned_text, max_chunk_length=500, stride=400)\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b401fa-dcce-464a-b205-63ce8d20ad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brady\\.vscode\\omnistudy-AI_Testing\\venv\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#Initialize retriever model\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\",\n",
    "    model_format=\"sentence_transformers\",\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "# import torch\n",
    "# #Initialize retriever model\n",
    "# from haystack.nodes import EmbeddingRetriever\n",
    "# retriever = EmbeddingRetriever(\n",
    "#     document_store=document_store,\n",
    "#     embedding_model=\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\",\n",
    "#     model_format=\"sentence_transformers\",\n",
    "#     top_k=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b55761-4193-4aec-abb4-19333bac4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What kind of Big O notation is used during a coding interview?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3524217d-5d4e-4acb-b404-ed8266637ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import DocumentSearchPipeline\n",
    "\n",
    "search_pipe = DocumentSearchPipeline(retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9980845-9371-48c6-a7b9-33fa916a8024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6988f79e73b24abf91d6399508d106a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documents': [<Document: {'content': \"(and many other companies). algorithm and coding problems form the \\nlargest component of the interview process. Think of these as problem-solving questions. The interviewer \\nis looking to evaluate your ability to solve algorithmic problems you haven't seen before. \\nVery often, you might get through only one question in an interview. Forty-five minutes is not a long time, \\nand it's difficult to get through several different questions in that time frame. \\nYou should do your best to talk out loud t\", 'content_type': 'text', 'score': 0.8446781335, 'meta': {'doc_type': 'vector'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3a4a4200b73a0aab07f1c7cbd9caa735'}>, <Document: {'content': \"s to explore what areas of technology you're familiar with. \\nNext, you fly to Seattle (or whichever office you're interviewing for) for four or five interviews with one or \\ntwo teams that have selected you based on your resume and phone interviews. You will have to code on a \\nwhiteboard, and some interviewers will stress other skills. Interviewers are each assigned a specific area to \\nprobe and may seem very different from each other. They cannot see the other feedback until they have \\nsubmitted\", 'content_type': 'text', 'score': 0.839912981, 'meta': {'doc_type': 'vector'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd6335099aa506c5c6556ad120d71bda7'}>], 'root_node': 'Query', 'params': {}, 'query': 'What happens during a coding interview?', 'node_id': 'Retriever'}\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "res = pipe.run(query=query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7d7b2-f1f4-4dcf-b076-935b913315bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document\n",
    "\n",
    "batch_size = 256\n",
    "total_doc_count = len(text_chunks)\n",
    "\n",
    "counter = 0\n",
    "docs = []\n",
    "for d in text_chunks:\n",
    "    doc = Document(\n",
    "        content = d\n",
    "    )\n",
    "    docs.append(doc)\n",
    "    counter += 1\n",
    "    if counter % batch_size == 0 or counter == total_doc_count:\n",
    "        embeds = retriever.embed_documents(docs)\n",
    "        for i, doc in enumerate(docs):\n",
    "            doc.embedding = embeds[i]\n",
    "        document_store.write_documents(docs)\n",
    "        docs.clear()\n",
    "    if counter == total_doc_count:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38dda222-c877-43f7-af3b-d73debe79aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser\n",
    "pipe= Pipeline()\n",
    "\n",
    "\n",
    "prompt =\"\"\"Synthesize a comprehensive answer from the following top_k most relevant paragraphs and the given question. \n",
    "                             Provide a clear and concise response that summarizes the key points and information presented in the paragraphs. \n",
    "                             Your answer should be in your own words and be no longer than 50 words. \n",
    "                             \\n\\n Paragraphs: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\"\"\"\n",
    "template = PromptTemplate(prompt=prompt,output_parser=AnswerParser())\n",
    "node = PromptNode(model_name_or_path=\"mistralai/Mistral-7B-v0.1\",default_prompt_template=template,api_key=huggingface_api_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de088f03-bb0b-4e36-a927-df362077af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser\n",
    "from haystack import Document\n",
    "\n",
    "pipe.add_node(component=node,name=\"prompt_node\",inputs=[\"Query\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c07b50b-069d-42fc-83ed-c487b5c627ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebee855fffe4137835335ab7c48bf9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Answer {'answer': ' \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['ca2a24af3ea1d88021571f06009656ac', '3b1e1c8d8a57c665a4819626a8165814'], 'meta': {'prompt': \"Synthesize a comprehensive answer from the following topk most relevant paragraphs and the given question. \\n                             Provide a clear and concise response that summarizes the key points and information presented in the paragraphs. \\n                             Your answer should be in your own words and be no longer than 50 words. \\n                             \\n\\n Paragraphs:  O is closer to what academics mean by 0, in that it would be seen as incorrect to describe printing an \\narray as O(N2). Industry would just say this is O(N). \\nFor this book, we will use big O in the way that industry tends to use it: By always trying to offer the tightest \\ndescription of the runtime. \\nBest Case, Worst Case, and Expected Case \\nWe can actually describe our runtime for an algorithm in three different ways. \\nCrackingTheCodinglnterview.com I 6th Edition \\n39 \\nVI I Big 0 \\nLet's look a n this. Some of the most common ones are O(log N), O(N log N), \\nO(N), O(N2) and 0( 2N). There's no fixed list of possible runtimes, though. \\nYou can also have multiple variables in your runtime. For example, the time to paint a fence that's w meters \\nwide and h meters high could be described as O ( wh). If you needed p layers of paint, then you could say \\nthat the time is O ( whp). \\nBig 0, Big Theta, and Big Omega \\nIf you've never covered big O in an academic setting, you can probably skip this  \\n\\n Question: What kind of Big O notation is used during a coding interview? \\n\\n Answer:\"}}>]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069223f22607409297e68daee1b67b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Document: {'content': \" O is closer to what academics mean by 0, in that it would be seen as incorrect to describe printing an \\narray as O(N2). Industry would just say this is O(N). \\nFor this book, we will use big O in the way that industry tends to use it: By always trying to offer the tightest \\ndescription of the runtime. \\nBest Case, Worst Case, and Expected Case \\nWe can actually describe our runtime for an algorithm in three different ways. \\nCrackingTheCodinglnterview.com I 6th Edition \\n39 \\nVI I Big 0 \\nLet's look a\", 'content_type': 'text', 'score': 0.827158302, 'meta': {'doc_type': 'vector'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ca2a24af3ea1d88021571f06009656ac'}>, <Document: {'content': \"n this. Some of the most common ones are O(log N), O(N log N), \\nO(N), O(N2) and 0( 2N). There's no fixed list of possible runtimes, though. \\nYou can also have multiple variables in your runtime. For example, the time to paint a fence that's w meters \\nwide and h meters high could be described as O ( wh). If you needed p layers of paint, then you could say \\nthat the time is O ( whp). \\nBig 0, Big Theta, and Big Omega \\nIf you've never covered big O in an academic setting, you can probably skip this \", 'content_type': 'text', 'score': 0.823150158, 'meta': {'doc_type': 'vector'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3b1e1c8d8a57c665a4819626a8165814'}>]\n"
     ]
    }
   ],
   "source": [
    "res=pipe.run(query=query,documents=search_pipe.run(query=query,params={\"Retriever\": {\"top_k\": 2}})['documents'])\n",
    "\n",
    "#Produced Answer\n",
    "print(res['answers'])\n",
    "#Context used\n",
    "print(search_pipe.run(query=query,params={\"Retriever\": {\"top_k\": 2}})['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2fb9f-22b1-4d97-bc90-a87f2aaba1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import Pipeline\n",
    "from haystack.schema import Document\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# pipeline.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "pipeline.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"Query\"])\n",
    "output = pipeline.run(query=\"what happens during a coding interview?\", documents=[Document(content_pieces[0]),Document(content_pieces[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2fddc7-91bf-47fb-bc08-68f881c1a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "[a.answer for a in output[\"answers\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
